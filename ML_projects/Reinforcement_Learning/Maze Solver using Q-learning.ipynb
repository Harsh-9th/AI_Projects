{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5690e686",
   "metadata": {},
   "source": [
    "<h1>Maze Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a20d6",
   "metadata": {},
   "source": [
    "Creating a Training an agent using Q-Learning to help it move around in a maze(shown below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba9da1",
   "metadata": {},
   "source": [
    "![alt text](<Maze_Pic-1.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "481ffbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the library\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13bafdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting parameters (alpha[learning rate] and gamma[discount factor])\n",
    "alpha=0.9\n",
    "\n",
    "gamma=0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b727381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining states\n",
    "locations_to_states= {\n",
    "    'A':0,\n",
    "    'B':1,\n",
    "    'C':2,\n",
    "    'D':3,\n",
    "    'E':4,\n",
    "    'F':5,\n",
    "    'G':6,\n",
    "    'H':7,\n",
    "    'I':8,\n",
    "    'J':9,\n",
    "    'K':10,\n",
    "    'L':11\n",
    "}\n",
    "\n",
    "#Defining actions\n",
    "actions=[0,1,2,3,4,5,6,7,8,9,10,11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9648cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the rewards\n",
    "rewards= np.array([[0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                   [1,0,1,0,0,1,0,0,0,0,0,0],\n",
    "                   [0,1,0,0,0,0,1,0,0,0,0,0],\n",
    "                   [0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                   [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "                   [0,1,0,0,0,0,0,0,0,1,0,0],\n",
    "                   [0,0,1,0,0,0,100,1,0,0,0,0],\n",
    "                   [0,0,0,1,0,0,1,0,0,0,0,1],\n",
    "                   [0,0,0,0,1,0,0,0,0,1,0,0],\n",
    "                   [0,0,0,0,0,1,0,0,1,0,1,0],\n",
    "                   [0,0,0,0,0,0,0,0,0,1,0,1],\n",
    "                   [0,0,0,0,0,0,0,1,0,0,1,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8abfe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Values:\n",
      "[[  0 170   0   0   0   0   0   0   0   0   0   0]\n",
      " [128   0 226   0   0 129   0   0   0   0   0   0]\n",
      " [  0 170   0   0   0   0 300   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 226   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  74   0   0   0]\n",
      " [  0 170   0   0   0   0   0   0   0  97   0   0]\n",
      " [  0   0 226   0   0   0 399 226   0   0   0   0]\n",
      " [  0   0   0 170   0   0 300   0   0   0   0 170]\n",
      " [  0   0   0   0  56   0   0   0   0  97   0   0]\n",
      " [  0   0   0   0   0 128   0   0  74   0 128   0]\n",
      " [  0   0   0   0   0   0   0   0   0  97   0 170]\n",
      " [  0   0   0   0   0   0   0 226   0   0 128   0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Initializing the Q-values\n",
    "Q=np.array(np.zeros([12,12]))\n",
    "\n",
    "#Implementing the Q-learning process\n",
    "for i in range(1000):\n",
    "    current_state= np.random.randint(0,12)\n",
    "    playable_actions= []\n",
    "\n",
    "    for j in range(12):\n",
    "        if rewards[current_state,j] > 0:\n",
    "            playable_actions.append(j)\n",
    "\n",
    "    next_state= np.random.choice(playable_actions)\n",
    "    #Temporal Difference(TD)\n",
    "    TD= rewards[current_state, next_state] + gamma*Q[next_state, np.argmax(Q[next_state, ])] - Q[current_state, next_state]\n",
    "\n",
    "    Q[current_state, next_state]= Q[current_state, next_state] + alpha * TD\n",
    "\n",
    "#checking the Q-matrix\n",
    "print(\"Q-Values:\")\n",
    "print(Q.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f4a0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a mapping from states to the locations\n",
    "\n",
    "state_to_location= {state: location for location, state in locations_to_states.items()}\n",
    "def route(starting_location, ending_location):\n",
    "    route=[starting_location]\n",
    "    next_location= starting_location\n",
    "    while (next_location != ending_location):\n",
    "        starting_state= locations_to_states[starting_location]\n",
    "        next_state= np.argmax(Q[starting_state, ])\n",
    "        next_location= state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        starting_location= next_location\n",
    "    return route\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36789135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E', 'I', 'J', 'F', 'B', 'C', 'G']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Route:')\n",
    "route('E', 'G')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb7419",
   "metadata": {},
   "source": [
    "<h2>Improved Version of the Above method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca99f80",
   "metadata": {},
   "source": [
    "Adding automatic reward distribution to make agent tmove to any desired block in the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the rewards (second time)\n",
    "rewards= np.array([[0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                   [1,0,1,0,0,1,0,0,0,0,0,0],\n",
    "                   [0,1,0,0,0,0,1,0,0,0,0,0],\n",
    "                   [0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                   [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "                   [0,1,0,0,0,0,0,0,0,1,0,0],\n",
    "                   [0,0,1,0,0,0,1,1,0,0,0,0],\n",
    "                   [0,0,0,1,0,0,1,0,0,0,0,1],\n",
    "                   [0,0,0,0,1,0,0,0,0,1,0,0],\n",
    "                   [0,0,0,0,0,1,0,0,1,0,1,0],\n",
    "                   [0,0,0,0,0,0,0,0,0,1,0,1],\n",
    "                   [0,0,0,0,0,0,0,1,0,0,1,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def route_imporoved(starting_location, ending_location):\n",
    "    rewards_new=np.copy(rewards)\n",
    "    ending_state=locations_to_states[ending_location]\n",
    "    rewards_new[ending_state, ending_state]= 1000\n",
    "    Q=np.array(np.zeros([12, 12]))\n",
    "\n",
    "    for i in range(1000):\n",
    "        current_state= np.random.randint(0, 12)\n",
    "        playable_actions=[]\n",
    "\n",
    "        for j in range(12):\n",
    "            if rewards_new[current_state, j] > 0:\n",
    "                playable_actions.append(j)\n",
    "        \n",
    "        next_state= np.random.choice(playable_actions)\n",
    "        TD= rewards_new[current_state, next_state] +  gamma * Q[next_state, np.argmax(Q[next_state, ])] - Q[current_state, next_state]\n",
    "        Q[current_state, next_state]= Q[current_state, next_state] + alpha * TD\n",
    "\n",
    "    route = [starting_location]\n",
    "    next_location= starting_location\n",
    "\n",
    "    while (next_location != ending_location):\n",
    "        starting_state= locations_to_states[starting_location]\n",
    "        next_state= np.argmax(Q[starting_state, ])\n",
    "        next_location= state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        starting_location= next_location\n",
    "    return route\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6609770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Route:\")\n",
    "route_imporoved('E', 'D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68d0da",
   "metadata": {},
   "source": [
    "\n",
    "Adding an intermediary state to make agent pass through the certan block before reaching the end goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_route(starting_location, intermediary_location, ending_location):\n",
    "    return route_imporoved(starting_location, intermediary_location) + route_imporoved(intermediary_location, ending_location)[1:]\n",
    "best_route('E', 'F', 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72823ba",
   "metadata": {},
   "source": [
    "Working:  \n",
    "#The program trains an agent to pass through a maze using Q learning. \n",
    " \n",
    "#Though Q learning is the easier side of reinforcement learning and is most effective in stochastic environments. However it struggles if state and action spaces are too large (also known as curse of dimensionality)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
